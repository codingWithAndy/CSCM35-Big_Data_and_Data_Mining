\documentclass[a4paper,10pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}

%Includes "References" in the table of contents
\usepackage[nottoc]{tocbibind}
\usepackage{titling}
\usepackage{setspace}

\parskip .8ex

\setlength{\droptitle}{-15em}

%Begining of the document
\begin{document}

\title{\textbf{CSCM35: Big Data and Data Mining \\Coursework 1}}
\date{09/04/20}
\author{Andy Gray\\445348}

\maketitle

\section{Introduction}

We have assigned, onto us, a practical task, that is related to the field of data mining. This task aims to use the association rule, a rule-based machine learning technique \cite{geron2019hands}, to discover interesting relationships within the provided large dataset. We will be creating code to process the data, as well as analysing the results to see if there are any insight gains and if there are any possible reasons to why these might be the case. 

Data mining is a necessary part of obtaining knowledge through discovery in databases (KDD). KDD is the term used for the overall process. Data mining tasks split into two main categories, which are predictive and descriptive tasks. However, these tasks split further into four core mining task. These are cluster analysis, predictive modelling, anomaly detection and association analysis \cite{tan2016introduction}. We will be focusing on the association analysis within this paper.

We will be applying the apriori and association rule to look at the data as a whole, to see if any patterns emerge comparing the results on the confidence and lift metrics. As well as analysing the itemset for individual countries, these countries are the United Kingdom (Uk), Germany, France and the Republic of Ireland (Éire). We chose these countries as they are the top 4 counties with the number of transactions recorded. We found that France and Éire buy their stock in big bulks, while the Uk buys in bulk but by not as much as Éire and France, while Germany tends to buy more individual items.

We will first look at the algorithms used within the proposed solution, explaining how they work and what are the maths formulas driving the algorithms. We will then explain the dataset, and the data preprocessing that occurred, followed by an explanation of the packages used and the parameters used for the algorithms. We will then explain and discuss the results to see if any insights are present. To end, we will be then concluding what we have found.

\section{Proposed Solution}

\subsection{Understanding the Problem}

What we need to do is take the provided dataset and perform appropriate data mining techniques on it. To try to find any patterns within it. We will achieve this by using appropriate data mining tools, techniques and algorithms.

To asses, if there are any distinctive patterns within the dataset, we will look at the results when focusing on different metrics as well as look at subsections, for example, country, within the data. We would expect to see a reduced list of items, displaying the antecedents items and the consequents of those items based on what metric we use.

\subsection{Packages}

We will be using the programming language Python 3 \cite{Python}, as this allows us to use all the required packages needed to analyse the dataset. With aiding in loading in the dataset, we will use the library Pandas library \cite{mckinney-proc-scipy-2010}. We will be using the library package MLXtend\cite{raschkas_2018_mlxtend} to be able to get access to the apriori and the association rule algorithm. We will be using Matplotlib's \cite{hunter2007matplotlib} package library for visualising our data, to allow us to be able to get insights and spot possible trends.

\subsection{Algorithms Used Explanation}

The first algorithm that we used is one that is from the frequent itemset mining methods, called Apriori \cite{han2011data}. Apriori is an unsupervised learning machine learning algorithm proposed by R. Agrawal and R. Srikant in 1994 \cite{agrawal1994fast, geron2019hands}. The algorithm focuses on using boolean association rules \cite{agrawal1994fast} from using prior knowledge of itemsets that contain the frequent properties. Apriori uses a level-wise search, which operates an iterative approach, where $k$-itemsets get used for exploring $(k+1)$-itemsets \cite{jingjingslides}. In order to improve efficiency, which will reduce the search space, an important characteristic called the Apriori property needs to be applied \cite{han2011data}. 

The Apriori property has a two-step process which involves the join and the prune step. For this explanation, $F_k$ represents the $k$-itemset where $L_k$ represents the candidate for the $k$-itemset. The process of joining is to generate a new itemset, $L_{k+1}$, from the $F_K$ itemset. While the pruning stage aims to identify the itemsets in $L_{k+1}$ that are infrequent from $k$, and then remove them \cite{jingjingslides}. What indicates if the item is infrequent depends on the support count, which is predefined beforehand. Therefore what the algorithm does, is: Let us assume that $k=1$ and a support count of 2, we generate a frequent itemset, at first 1, which we will refer to as $F_1$. What this is doing is scanning the dataset to figure out the count of each occurrence of each item. The next step is the merge, or join, the datasets. Using $F_k$ we can then create $L_{k+1}$. We then prune the data based on the support count eliminating any data that is infrequent, therefore leaving any data that is classed as frequent, adding it to $F_{k+1}$. This process is repeated until $F_k$ is empty \cite{jingjingslides, han2011data}.

The second algorithm that we have used is called the association rule. Rakesh Agrawal, Tomasz Imieli{\'n}ski and Arun Swami developed the algorithm in 1993 \cite{agrawal1993mining}. The association rule algorithm is an unsupervised machine learning algorithm \cite{geron2019hands}. What this algorithm focuses around is the support of the datasets' items and the confidence of the association. The math formula for the support is $support(A \Rightarrow B) = P(A \cup B)$, and the math formula for the confidence is $confidence(A \Rightarrow B) = P(B|A)$.  Similar to the apriori, the support count will drop any relationships that do not meet the desired count. The formula to figure out if the relationships meet the support count is $confidence(A \Rightarrow B) = P(B|A) = \frac{support(A \cup B)}{support(A)} = \frac{support\_count(A \cup B)}{support\_count(A)}$ \cite{jingjingslides, han2011data}. However, the association rule relies on a procedure, like the apriori algorithm, to have been implemented on the dataset first before it can work effectively. While the association rule requires the support threshold, the confidence level, which we can use to make decisions based on the links, can be changed to additional metrics. The metric can be several different ones like conviction and leverage, but the other one to the confidence that we will focus on is the lift metric. The metric lift was introduced in 1997 by Sergey Brin, Rajeev Motwani, Jefferey D. Ullman and Shalom Tsur \cite{brin1997dynamic}. This metric figures out how the antecedent and consequent of a rule, $A \rightarrow C$, would occur together and not as statistically independent items. The lift score would indicate if A and C are independent by having a score of exactly 1. The math formula for lift is constructed as $lift(A \rightarrow C) = \frac{confidence(A \rightarrow C)}{support(C)}, range[0, \infty]$ \cite{brin1997dynamic, jingjingslides}.

Overall the apriori algorithm will reduce the dataset by pruning it. The amount of pruning depends on the support count threshold that is applied. The output will create the required frequent itemset which the association rule requires. The association rule will then go through the frequent itemset to acquire any patterns of items based on the support count and the metric. In our case, this is the lift or confidence metric.

\subsection{Dataset and Data Preprocessing}

The dataset we have acquired is a shopping dataset. It is 44MB in size and is in the format of CSV. There are eight attributes, within the dataset, with 541,910 records. The attributes are InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerId, Country. There are 4,335 unique customers, 1,8405 individual invoices, 3,659 unique stock items and 37 unique countries. 

The purpose of data preprocessing is to convert any raw data into a format that is appropriate for the following analysis of the data. Preprocessing can involve fusing data from several sources, as well as cleaning the raw data to remove any noise, duplicate observations or ambiguity \cite{tan2016introduction}. The main aim of the preprocessing is to get data that is accurate, complete and consistent, but in the real world, we will usually get inaccurate, incomplete and inconsistent data \cite{han2011data}. The preprocessing stage can also involve just selecting the essential records and features that are desired and are relevant to the set data mining task \cite{tan2016introduction}. We can now see that the main aim of data processing is to clean the data, we achieve this through filling in missing values, identifying or removing outliers, smoothing noisy data, and resolving and data inconsistencies \cite{han2011data}.

The dataset had values missing in a number of the columns. The rows that had any missing values, within the features, were removed from the dataset. Also, any rows that had data that was an outlier, within its features, was removed from the dataset. These outliers included minus values. Once we had carried out these data cleaning actions, we then have 396,371 records remaining. The cleaning process indicates that we had removed a total of 145,539 records from the dataset. 

Before we could give the apriori algorithm the dataset, we have to perform a data transfer on the dataset. First, we placed the required features into a basket and then performed the data transfer function on it, converting the values into binary values. Grouping the data by quantity using the InvoiceNo and Description feature and then index the values using the InvoiceNo. We then used this basket to feed into the apriori algorithm to create our frequent items dataset.

When we were analysing the data set based on country, we used the same process to transfer the data. However, we had another parameter for the basket that only selected the required data for that country, feeding that basket into the algorithm.


\subsection{Parameters}

When using the apriori algorithm on the whole dataset, we set the minimal support to 0.2, and we passed through no country filter. We then performed the association rule using both the lift and confidence metric. The minimum support for the lift was the value 5, and for the confidence, we used the value 0.5.

When analysing the dataset by individual countries, we looked at the countries the United Kingdom, Germany, France, Republic of Ireland (Éire) and Spain. When we performed the apriori algorithm, we used a minimum support level of 0.03 for all of the countries, except the UK, which had 0.02. When looking at the countries association rules using the lift metric, we used minimum support of 10 for all except for Germany, where we used a value of 5. For the confidence metric, we applied a minimum support level of 0.5 to all of the countries.

\subsection{Visual and Statistical Analysis}

When looking at the bar chart in appendix \ref{appendix:totalcountall}, we can see that the United Kingdom has the most number of counts. The United Kingdom has a count of 345,005 which is then followed by Germany, with 8,659, France 8,034, Éire on 7,138 and Spain with 2,424 making up the top five. With the rest being between the ranges 2,326 and 9.

When applying the association rule with the lift metric (see appendix \ref{appendix:wholelift}), the antecedents with the hight lift value is 23.863 with the consequents of Roses Regency and the Green Regency teacup and saucer. This result demonstrates that there is a strong link between the antecedents and consequents. However, when looking at the whole dataset with the confidence metric used (see appendix \ref{appendix:wholeconf}), the antecedents Roses and Pink Regency teacups and saucers had a confidence level of 0.894 that a consequent of Green Regency Teacup and saucer. This score indicates a high likelihood that someone buying these items will buy the Green teacup set.

When we look at the UK's data (see appendix \ref{appendix:uklift}), we can see that the items with the highest lift are 'Green Regency Teacup and Saucer' with a consequence of 'Pink Regency Teacup and Saucer and Roses Regency Teacup and Saucer'. However, this only has a confidence level of 0.557, while if they are the other way round the confidence level of the Green set being the consequence is 0.89. The table shows that there is a strong link between these items whatever the order they are in, all being around the 0.85 confidence but the most like combination is the one stated previously. These results are evident within the results focusing on the data when the confidence metric is applied. [create appendix for these two results]

When we look at the data for Germany, it is clear that the antecedents and consequents are one to one item mapping (see appendix \ref{appendix:germanylift}). The antecedents and conquests are also very similar items, 'Spaceboy Cup' matched with 'Spaceboy Bowl', as well as 'Set/6 Red spotty paper cups' with Set/6 red spotty paper plates'. They are showing a strong link between item similarity. However, when looking at the confidence (see appendix \ref{appendix:germanyconf}), the antecedents items 'Red retrospot Charlotte bag' and 'Round snack boxes set of 4 Woodland' has a value of 1.00 for the consequent 'Woodland Charlotte bag' but only has a lift value of 7.63, which is about mid-level compared to the others.

The data for France (see appendix \ref{appendix:francelift}) shows that all the links for the lift are all around the items 'set of 20 red retrospot paper napkins', 'pack of 6 skull paper plates', 'set of 6 spotty paper cups' for antecedents. With the consequence of 'set of 6 spotty paper plates', 'pack of 20 skull paper napkins' which has a lift value of 24.989. The rest are based around these items, alternating between being the antecedents and consequents. Again when reviewing the confidence metric, The same items are near the top with a confidence value of 1 (see appendix \ref{appendix:franceconf}).

In regards to Ireland, the items at the top of the lift table (see appendix \ref{appendix:eirelift}), with a value of 25.283, is dominated by the same items that can interchange between antecedents and consequents. These are 'green regency teacup and saucer', 'regency sugar bowl green', 'regency cakestand 3 tier', 'regency tea plate pink', 'Regency tea plate green', 'Pink regency teacup and saucer', 'regency milk jug pink'.  These items are also what dominates the confidence table (see appendix \ref{appendix:eireconf}), with confidence support value of 1.


\section{Discussion and Conclusion}

When comparing the countries transactions, Germany's results indicate that many items get bought individually, or in small bulks, as there are not many items required to provide a consequent, especially for the lift. However, places like France and especially Éire will have about 3 to 5 antecedents to provide a consequent, hinting that many items get bought together often, probably bought in bulk. Compared to other countries, Germany buy very different types of items. The other countries have trends related to teacups and saucers, which is similar to the dataset as a whole, while Éire items are the same items but in green. While Germany is more based around woodland bags and children related items like paper cups and plates.

To conclude, in order to run the apriori algorithm, we first needed to prepare the dataset and carry out preprocessing, which involved cleaning the data from any records that were deemed inappropriate for analysing. Once we have done this, we can then place the data into buckets to then encode it and then pass it through the apriori algorithm which then, by using minimum support of 0.5, give us a frequent itemset. Using the frequent itemset with the association rule algorithm provided us with a list of antecedents and consequents items, depending on what metric, lift or confidence, was assigned. From these results, we could see that three of the countries had very similar outcomes as that of the dataset as a whole. However, Germany was different, having single items rather than several items providing a likely consequence.

\medskip
\newpage
\begin{appendices}
	\section{Total count for Country}
	\label{appendix:totalcountall}
	%% Add image of graph here.
	\includegraphics[scale=0.5]{totalcount}
	
	\section{Lift Table of Items Whole Dataset}
	\label{appendix:wholelift}
	\includegraphics[scale=0.2]{wholelift}
	
	\section{Confidence Table of Items Whole Dataset}
	\label{appendix:wholeconf}
	\includegraphics[scale=0.2]{wholeconf}
	
	\section{Lift Table of United Kingdom Items}
	\label{appendix:uklift}
	%% Add image of graph here.
	\includegraphics[scale=0.2]{uklift}
	
	\section{Confidence Table of United Kingdom Items}
	\label{appendix:ukconf}
	\includegraphics[scale=0.2]{ukconf}
	
	\section{Lift Table of Germany Items}
	\label{appendix:germanylift}
	\includegraphics[scale=0.2]{germanylift}
	
	\section{Confidence Table of Germany Items}
	\label{appendix:germanyconf}
	\includegraphics[scale=0.2]{germanyconf}
	
	\section{Lift Table of France Items}
	\label{appendix:francelift}
	\includegraphics[scale=0.2]{francelift}
	
	\section{Confidence Table of France Items}
	\label{appendix:franceconf}
	\includegraphics[scale=0.2]{franceconf}
	
	\section{Lift Table of Erie Items}
	\label{appendix:eirelift}
	\includegraphics[scale=0.19]{eirelift}
	
	\section{Confidence Table of Eire Items}
	\label{appendix:eireconf}
	\includegraphics[scale=0.19]{eireconf}
	
\end{appendices}

\newpage

%Sets the bibliography style to UNSRT and imports the 
%bibliography file "samples.bib".
\bibliographystyle{acm}
\bibliography{samples}

\end{document}