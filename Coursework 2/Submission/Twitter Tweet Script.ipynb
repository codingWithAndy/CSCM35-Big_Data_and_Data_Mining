{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrI81Xn6wh0d",
        "colab_type": "text"
      },
      "source": [
        "# **Notebook to \"Hydrate\" tweets-ID**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbVmAxxGHUjO",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeimRLrjwpRr",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "d493b5fd-5c44-4f23-d8a9-d9ac0ee4e799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "dirpath = os.getcwd()\n",
        "print(\"current directory is : \" + dirpath)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "current directory is : /Users/Andy/Desktop/covid19-tweets-dataset/COVID19_Tweets_Dataset\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEsz47hdFHtW",
        "colab_type": "text"
      },
      "source": [
        "### Twitter API Keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et0_5DEEFNpW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# Code adapted from: https://www.kaggle.com/lopezbec/covid19-tweets-dataset\n",
        "#@title Insert API Keys here { run : \"auto\"}\n",
        "from twarc import Twarc\n",
        "\n",
        "# These keys are received after applying for a twitter developer account\n",
        "\n",
        "consumer_key = \"\" #@param {type:\"string\"}\n",
        "consumer_secret = \"\" #@param {type:\"string\"}\n",
        "access_token = \"\" #@param {type:\"string\"}\n",
        "access_token_secret = \"\" #@param {type:\"string\"}\n",
        "\n",
        "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASdYyTqI9RUG",
        "colab_type": "text"
      },
      "source": [
        "# Configuration: Choose Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fj9ydtn9aIL",
        "colab_type": "text"
      },
      "source": [
        "### Keywords\n",
        "\n",
        "We recommend you run the code for a few keywords and create many output.csv files. Then, you can copy them to a different directory, and merge them when  required. Selecting too many keywords and dates might result in data too large to be stored in memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGdsBbsHxrcG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Check Keywords to Hydrate { run: \"auto\" }\n",
        "coronavirus = True #@param {type:\"boolean\"}\n",
        "virus = False #@param {type:\"boolean\"}\n",
        "covid = False #@param {type:\"boolean\"}\n",
        "ncov19 = False #@param {type:\"boolean\"}\n",
        "ncov2019 = False #@param {type:\"boolean\"}\n",
        "keyword_dict = {\"coronavirus\": coronavirus, \"virus\": virus, \"covid\": covid, \"ncov19\": ncov19, \"ncov2019\": ncov2019}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSNFUOZN952G",
        "colab_type": "text"
      },
      "source": [
        "### Get Number of Tweets by Dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u1DI1_oX6TV",
        "colab_type": "text"
      },
      "source": [
        "If you are running this in Google Colab we recommend starting with a range of just 1 day, do to the Google Colab 12hr limit and Twitter API limit. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-UwuBDZ-xW1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter range of dates to Hydrate { run: \"auto\" }\n",
        "start_date = '2020-01-22' #@param {type:\"date\"}\n",
        "end_date = '2020-02-28' #@param {type:\"date\"}\n",
        "\n",
        "import datetime as dt\n",
        "files = []\n",
        "covid_loc = \"COVID19_Tweets_Dataset\"\n",
        "# Looks at each volder\n",
        "for folder in os.listdir(covid_loc):\n",
        "    foldername = os.fsdecode(folder)\n",
        "    # The folder name is a keyword. We continue for keywords selected above\n",
        "    if keyword_dict.get(foldername.split()[0].lower()) == True:\n",
        "        folderpath = os.path.join(covid_loc, foldername)\n",
        "        # Each file is of the format [keyword]_yyyy_mm_dd.txt\n",
        "        for file in os.listdir(folderpath):\n",
        "            filename = os.fsdecode(file)\n",
        "            date = filename[filename.index(\"_\")+1:filename.index(\".\")]\n",
        "\n",
        "            # If the date is within the required range, it is added to the\n",
        "            # list of files to read.\n",
        "            if (dt.datetime.strptime(start_date, \"%Y-%m-%d\").date() \n",
        "            <= dt.datetime.strptime(date, '%Y_%m_%d').date()\n",
        "             <= dt.datetime.strptime(end_date, \"%Y-%m-%d\").date()):\n",
        "                files.append(os.path.join(folderpath, filename))\n",
        "# The final list is read, and each of the individual IDs is stored in a collective\n",
        "# set of IDs. Duplicates are removed.\n",
        "ids = set()\n",
        "for filename in files:\n",
        "    with open(filename) as f:\n",
        "        # The files are of the format: [id1,id2,id3,...,idn]\n",
        "        # Remove the brackets and split on commas\n",
        "        for i in f.readline().strip('][').replace(\" \", \"\").split(\",\"):\n",
        "            ids.add(i) \n",
        "# Number of tweets read.\n",
        "print(round((len(ids)/1000000), 3), \"million unique tweets.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.857 million unique tweets.\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BDEuBH7LPTn",
        "colab_type": "text"
      },
      "source": [
        "### Save configuration into a file\n",
        "All the IDs are read into a single set in the previous code block using the specified configuration. The ID Output file stores all the IDs in a single file so that the configuration blocks don't have to be run again. In case the program unexpectedly stops, you can just run the code for Initialization and then the code for Hydration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4EfPTrnLO0c",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter ID output file {run: \"auto\"}\n",
        "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
        "# The set of IDs is stored in this file.\n",
        "with open(final_tweet_ids_filename, \"w+\") as f:\n",
        "    for id in ids:\n",
        "        f.write('%s\\n' % id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxFa0jOTKbBw",
        "colab_type": "text"
      },
      "source": [
        "# Hydrate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBBH-a4WK1JM",
        "colab_type": "text"
      },
      "source": [
        "### Set up output file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U74uUnemI6h9",
        "colab_type": "text"
      },
      "source": [
        "The final_tweet_ids_filename should be exactly the same as the ID output file from the Configuration block. If this file does not exist in the working directory, you have to re-run the Configuration block.\n",
        "\n",
        "Please also keep the output_filename the same in case the code is halted. That way, tweets already hydrated aren't re-hydrated for no reason. \n",
        "\n",
        "Also, please do not remove the .txt file created after running the Hydrate block until all the data is converted to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ATxyEfSLBK1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Set up Directory { run: \"auto\"}\n",
        "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
        "output_filename = \"output.csv\" #@param {type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG9cS-aoW0Wy",
        "colab_type": "text"
      },
      "source": [
        "The time for this code will depend on how many tweets you want to “hydrate”. Also, be advise of the Tweet API limit, the code will “go to sleep” once the limit is reach and automatically continue. \n",
        "You can leave this code running in Google Colab for a max of 12hrs. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFwYd7m58WR3",
        "colab": {}
      },
      "source": [
        "import jsonlines, json\n",
        "# Stores hydrated tweets here as jsonl objects\n",
        "# Contains one json object per line\n",
        "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
        "ids = []\n",
        "with open(final_tweet_ids_filename, \"r\") as ids_file:\n",
        "    ids = ids_file.read().split()\n",
        "hydrated_tweets = []\n",
        "ids_to_hydrate = set(ids)\n",
        "\n",
        "# Looks at the output file for already hydrated tweets\n",
        "if os.path.isfile(output_json_filename):\n",
        "    with jsonlines.open(output_json_filename, \"r\") as reader:\n",
        "        for i in reader.iter(type=dict, skip_invalid=True):\n",
        "            # These tweets have already been hydrated. So remove them from ids_to_hydrate\n",
        "            hydrated_tweets.append(i)\n",
        "            ids_to_hydrate.remove(i[\"id_str\"])\n",
        "print(\"Total IDs: \" + str(len(ids)) + \", IDs to hydrate: \" + str(len(ids_to_hydrate)))\n",
        "print(\"Hydrated: \" + str(len(hydrated_tweets)))\n",
        "\n",
        "count = len(hydrated_tweets)\n",
        "start_index = count # The index from where tweets haven't been saved to the output_json_file\n",
        "# Stores hydrated tweets to output_json_file every num_save iterations.\n",
        "num_save  = 1000\n",
        "\n",
        "# Now, use twarc and start hydrating\n",
        "for tweet in t.hydrate(ids_to_hydrate):\n",
        "    hydrated_tweets.append(tweet)\n",
        "    count += 1\n",
        "    # If num_save iterations have passed,\n",
        "    if (count % num_save) == 0:\n",
        "        # Open the output file\n",
        "        # NOTE: Even if the code stops during IO, only tweets from the current iteration are lost.\n",
        "        # Older tweets are preserved as the file is written in append mode.\n",
        "        with jsonlines.open(output_json_filename, \"a\") as writer:\n",
        "            print(\"Started IO\")\n",
        "            # Now write the tweets from start_index. The other tweets don't have to be written\n",
        "            # as they were already written in a previous iteration or run.\n",
        "            for hydrated_tweet in hydrated_tweets[start_index:]:\n",
        "                writer.write(hydrated_tweet)\n",
        "            print(\"Finished IO\")\n",
        "        print(\"Saved \" + str(count) + \" hydrated tweets.\")\n",
        "        # Now, since everything has been written. Reset start_index\n",
        "        start_index = count\n",
        "# There might be tweets unwritten in the last iteration if the count is not a multiple of num_tweets.\n",
        "# In that case, just write out the remainder of tweets.\n",
        "if count != start_index:\n",
        "    print(\"Here with start_index\", start_index)\n",
        "    with jsonlines.open(output_json_filename, \"a\") as writer:\n",
        "        for hydrated_tweet in hydrated_tweets[start_index:]:\n",
        "           writer.write(hydrated_tweet)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6O-KWKnbcGt",
        "colab_type": "text"
      },
      "source": [
        "## Convert JSONL to CSV\n",
        "Data is stored in  output_json_file from the previous code block. This now converts the jsonl .txt file into a csv file. Note that the column names required is stored as a list in the code.\n",
        "\n",
        "Note that a few of the columns are actually json objects (for example, user or entities). You will have to clean these objects into 1D data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x68cvh5AJCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert jsonl to csv\n",
        "import csv, jsonlines\n",
        "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
        "# These are the column name that are selected to be stored in the csv\n",
        "keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
        "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
        "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
        "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
        "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
        "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
        "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
        "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
        "          \"quoted_status_permalink\"]\n",
        "hydrated_tweets = []\n",
        "# Reads the current tweets\n",
        "with jsonlines.open(output_json_filename, \"r\") as reader:\n",
        "    for i in reader.iter(type=dict, skip_invalid=True):\n",
        "        hydrated_tweets.append(i)\n",
        "# Writes them out\n",
        "with  open(output_filename, \"w+\") as output_file:\n",
        "    d = csv.DictWriter(output_file, keyset)\n",
        "    d.writeheader()\n",
        "    d.writerows(hydrated_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvrm8g7NHNs",
        "colab_type": "text"
      },
      "source": [
        "Your data is now stored in output_filename. If you want to re-run  this notebook, please copy output_filename file to a different directory."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Automatically_Hydrate_TweetsIDs_COVID19.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DbVmAxxGHUjO",
        "G-Zr5kB6wknZ",
        "RDzd7FUKFviv"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}